{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demolib import spark\n",
    "from demolib.streams import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition\n",
    "data_dir = '/Sandbox/notebooks/SparkTheDefinitiveGuide/SparkTheDefinitiveGuide/data/activity-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(data_dir)\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    "        .json(data_dir)\n",
    "activityCounts = streaming.groupBy(\"gt\").count()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.count(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active.count(spark.streams.active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------+-------------------+\n",
      "|      gt| model| arrival_time|      creation_time|\n",
      "+--------+------+-------------+-------------------+\n",
      "|stairsup|nexus4|1424687983719|1424687981726802718|\n",
      "|stairsup|nexus4|1424687984000|1424687982009853255|\n",
      "|stairsup|nexus4|1424687984404|1424687982411977009|\n",
      "|stairsup|nexus4|1424687984805|1424687982814351277|\n",
      "|stairsup|nexus4|1424687985210|1424687983217500861|\n",
      "+--------+------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"ccfraud\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"ccfraud\")\\\n",
    "    .option('startingOffsets', -2)\n",
    "    .option('endingOffsets', -1)\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demolib import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"ccfraud\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(r):\n",
    "\n",
    "query = df_kafka.writeStream.foreach(process_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = df_kafka.writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_batch(batchDF, batch_id):\n",
    "    batchDF.persist()\n",
    "    batchDF.write.format('csv').save('../data/kafka/{}/out'.format(batch_id))\n",
    "    batchDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x2cced8e85c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.foreachBatch(persist_batch) \\\n",
    "    .outputMode(\"append\")  \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka.toDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query = df_kafka.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_kafka.writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Streaming from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demolib import spark, Namespace\n",
    "from demolib.streams import *\n",
    "\n",
    "kafkacfg = Namespace\n",
    "kafkacfg.table = \"ccfraud_stream\"\n",
    "kafkacfg.topic = 'ccfraud'\n",
    "kafkacfg.bootstrap = 'localhost:9092'\n",
    "kafkacfg.groupid = 'sstreaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafkacfg.bootstrap) \\\n",
    "  .option(\"subscribe\", kafkacfg.topic) \\\n",
    "  .option(\"group.id\", kafkacfg.groupid) \\\n",
    "  .load()\n",
    "#  .option(\"startingOffsets\", \"earliest\") \\\n",
    "\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df.writeStream\\\n",
    "    .queryName(kafkacfg.table)\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "|        |ccfraud_stream|       true|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT COUNT(*) FROM {kafkacfg.table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "| key|               value|  topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   119|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   120|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   121|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   122|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   123|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   124|2019-09-02 21:14:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   125|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   126|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   127|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   128|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   129|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   130|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   131|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   132|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   133|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   134|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   135|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   136|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   137|2019-09-02 21:15:...|            0|\n",
      "|null|[7B 22 63 63 5F 6...|ccfraud|        0|   138|2019-09-02 21:15:...|            0|\n",
      "+----+--------------------+-------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM {kafkacfg.table}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streams_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ccfraud_stream stopped\n"
     ]
    }
   ],
   "source": [
    "streams_stop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x25f42c89f28>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spark.streams.active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink To File with foreachBatch Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafkacfg.bootstrap) \\\n",
    "  .option(\"subscribe\", kafkacfg.topic) \\\n",
    "  .option(\"group.id\", kafkacfg.groupid) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_batch(batchDF, batch_id):\n",
    "    batchDF.persist()\n",
    "    batchDF.write \\\n",
    "        .mode('append') \\\n",
    "        .format('csv').save('../data/kafka/out'.format(batch_id))\n",
    "    batchDF.unpersist()\n",
    "\n",
    "query = df.coalesce(1).writeStream \\\n",
    "    .trigger(processingTime = '20 seconds') \\\n",
    "    .foreachBatch(persist_batch) \\\n",
    "    .outputMode(\"append\")  \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query None stopped\n"
     ]
    }
   ],
   "source": [
    "streams_stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink to MongoDB with foreachBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demolib.mongo import *\n",
    "\n",
    "static = spark.read.json(data_dir)\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "file_stream = spark.readStream \\\n",
    "    .schema(dataSchema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json(data_dir) \\\n",
    "    .withColumn('_id', expr(\"Device || ':' || Index || ':' || User\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mongo_persist_df(batchDF, batch_id):\n",
    "    \"\"\"Persist all records from batch_df into MongoDB collection\"\"\"\n",
    "    batchDF.persist()\n",
    "    mongo_save(batchDF, \"sensors\", database=\"test\")\n",
    "    batchDF.unpersist()\n",
    "\n",
    "# Create write stream query which uses foreachBatch as sink\n",
    "query = file_stream.coalesce(1).writeStream \\\n",
    "    .queryName('mongo_sink') \\\n",
    "    .trigger(processingTime = '0 seconds') \\\n",
    "    .foreachBatch(mongo_persist_df) \\\n",
    "    .outputMode(\"append\")  \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streams_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query mongo_sink stopped\n"
     ]
    }
   ],
   "source": [
    "streams_stop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
