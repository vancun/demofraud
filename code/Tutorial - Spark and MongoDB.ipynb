{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demolib import Namespace\n",
    "app_config = Namespace({\n",
    "    'database': 'test',\n",
    "    'collection': 'contacts',\n",
    "    'mongo_uri': 'mongodb://127.0.0.1',\n",
    "    'mongo_uri_collection': 'mongodb://127.0.0.1/test.contacts',\n",
    "    'spark_packages': ['org.mongodb.spark:mongo-spark-connector_2.11:2.4.1']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "    .config('spark.jars.packages', ','.join(app_config.spark_packages)) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to MongoDB Using Spark DataFrame API\n",
    "\n",
    "Let's start first with creating some sample data and putting it into a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = spark.createDataFrame([\n",
    "    (\"Bilbo Baggins\",  50), \n",
    "    (\"Gandalf\", 1000), \n",
    "    (\"Thorin\", 195), \n",
    "    (\"Balin\", 178), \n",
    "    (\"Kili\", 77),\n",
    "    (\"Dwalin\", 169),\n",
    "    (\"Oin\", 167), \n",
    "    (\"Gloin\", 158), \n",
    "    (\"Fili\", 82), \n",
    "    (\"Bombur\", None)\n",
    "], \n",
    "    [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+\n",
      "|         name| age|\n",
      "+-------------+----+\n",
      "|Bilbo Baggins|  50|\n",
      "|      Gandalf|1000|\n",
      "|       Thorin| 195|\n",
      "|        Balin| 178|\n",
      "|         Kili|  77|\n",
      "|       Dwalin| 169|\n",
      "|          Oin| 167|\n",
      "|        Gloin| 158|\n",
      "|         Fili|  82|\n",
      "|       Bombur|null|\n",
      "+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.format(\"mongo\") \\\n",
    "    .option(\"uri\", app_config.mongo_uri) \\\n",
    "    .option(\"database\", app_config.database)  \\\n",
    "    .option(\"collection\", app_config.collection) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above write we specified that Spark uses `overwrite` mode. This means the entire collection will be replaced. \n",
    "\n",
    "We could specify `append` mode. In such case Spark will perform upserts - documents with matching `_id` field in the database will be updated, documents without matching `_id` will be inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from Mongo Using Spark DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\", app_config.mongo_uri) \\\n",
    "    .option(\"database\", app_config.database) \\\n",
    "    .option(\"collection\", app_config.collection) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|                 _id| age|         name|\n",
      "+--------------------+----+-------------+\n",
      "|[5d6a18bd0bd95c18...|1000|      Gandalf|\n",
      "|[5d6a18bd0bd95c18...| 178|        Balin|\n",
      "|[5d6a18bd0bd95c18...|  77|         Kili|\n",
      "|[5d6a18bd0bd95c18...|  50|Bilbo Baggins|\n",
      "|[5d6a18bd0bd95c18...| 158|        Gloin|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode the database and collection names in the MongoDB URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------+\n",
      "|                 _id| age|   name|\n",
      "+--------------------+----+-------+\n",
      "|[5d6a19490bd95c18...| 167|    Oin|\n",
      "|[5d6a19490bd95c18...|1000|Gandalf|\n",
      "|[5d6a19490bd95c18...| 178|  Balin|\n",
      "+--------------------+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"mongo\") \\\n",
    "    .option(\"uri\",app_config.mongo_uri_collection) \\\n",
    "    .load()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsert Documents\n",
    "\n",
    "First we need to add document ID field, named `_id`.\n",
    "\n",
    "When writing to the collection MongoDB will use the `_id` field to update existing documents. If document doesn't exist, it will be inserted.\n",
    "\n",
    "The default update behavior is to replace the entire document. If we want to update only changed or new fields, we can setthe `replaceDocument` option value to `False` (default is `True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "people_with_id = people.withColumn('_id', col('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_with_id.write.format(\"mongo\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1\") \\\n",
    "    .option(\"database\", \"test\")  \\\n",
    "    .option(\"collection\", \"contacts_id\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
