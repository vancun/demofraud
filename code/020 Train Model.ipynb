{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_pipeline(schema, columns):\n",
    "    \"\"\"Create preprocessing transformation pipeline.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.types import StructType, StringType, NumericType\n",
    "    from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "\n",
    "    feature_columns = []\n",
    "    preprocessing_stages = []\n",
    "    for field in [f for f in schema.fields if f.name in columns]:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "\n",
    "            indexer = StringIndexer()\n",
    "            indexer.setInputCol(field.name).setOutputCol(f\"{field.name}_indexed\")\n",
    "            preprocessing_stages.append(indexer)\n",
    "\n",
    "            encoder = OneHotEncoder()\n",
    "            encoder.setInputCol(f\"{field.name}_indexed\").setOutputCol(f\"{field.name}_encoded\")\n",
    "            preprocessing_stages.append(encoder)\n",
    "\n",
    "            feature_columns.append(f\"{field.name}_encoded\")\n",
    "\n",
    "        elif isinstance(field.dataType, NumericType):\n",
    "            feature_columns.append(field.name)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    assembler = VectorAssembler()\n",
    "    assembler.setInputCols(feature_columns).setOutputCol('features')\n",
    "    preprocessing_stages.append(assembler)\n",
    "\n",
    "    return preprocessing_stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_dataframe(df, reduction_count):\n",
    "    \"\"\"There will be more non-fraud transactions than fraud transactions.\n",
    "    We will use K-Means algorithm to create non-fraud dataframe which matches\n",
    "    the size of the non-fraud transactions dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "\n",
    "    df = nonfraud_df\n",
    "    reduction_count = fraud_count\n",
    "\n",
    "    k_means = KMeans().setK(reduction_count).setMaxIter(30)\n",
    "    k_means_model = k_means.fit(df)\n",
    "\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    balanced_list = map(lambda v: (Vectors.dense(v[0:]) ,0.0), k_means_model.clusterCenters())\n",
    "    balanced_df = spark.createDataFrame(balanced_list, schema=['features', 'label'])\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_forest_model(df):\n",
    "    \"\"\"Create Random Forest Model from DataFrame\"\"\"\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "    (training, test) = df.randomSplit([0.7,0.3])\n",
    "    random_forest_estimator = RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"features\").setMaxBins(700)\n",
    "    model = random_forest_estimator.fit(training)\n",
    "    transaction_with_prediction = model.transform(test)\n",
    "    \n",
    "    import sys\n",
    "    sys.stderr.write(\"total data count is %d\\n\" % transaction_with_prediction.count())\n",
    "    sys.stderr.write(\"count where label matches prediction %d\\n\" % transaction_with_prediction.where(\"prediction = label\").count())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize app config\n",
    "import importlib\n",
    "from demolib import schema, cfg, spark\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fraud transactions from MongoDB\n",
    "fraud_df = spark.read.format(\"mongo\") \\\n",
    "    .options( uri = cfg.db.uri,\n",
    "              database = cfg.db.name,\n",
    "              collection = cfg.db.fraud\n",
    "            ) \\\n",
    "    .load()\n",
    "\n",
    "# Load non-fraud transactions from MongoDB\n",
    "nonfraud_df = spark.read.format(\"mongo\") \\\n",
    "    .options( uri = cfg.db.uri,\n",
    "              database = cfg.db.name,\n",
    "              collection = cfg.db.nonfraud\n",
    "            ) \\\n",
    "    .load()\n",
    "\n",
    "# Combine both fraud and non-fraud transactions using union\n",
    "transaction_df = fraud_df.unionAll(nonfraud_df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save preprocessing transforming model\n",
    "\n",
    "column_names = (\"cc_num\", \"category\", \"merchant\", \"distance\", \"amt\", \"age\")\n",
    "\n",
    "pipeline_stages = create_feature_pipeline(transaction_df.schema, column_names)\n",
    "pipeline = Pipeline().setStages(pipeline_stages)\n",
    "\n",
    "preprocessing_transformer_model = pipeline.fit(transaction_df)\n",
    "preprocessing_transformer_model.write().overwrite().save(cfg.model.preprocessing.path)\n",
    "\n",
    "feature_df = preprocessing_transformer_model.transform(transaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split fraud from nonfraud to create balanced dataframe\n",
    "\n",
    "fraud_df = feature_df \\\n",
    "  .filter(\"is_fraud = 1\") \\\n",
    "  .withColumnRenamed(\"is_fraud\", \"label\") \\\n",
    "  .select(\"features\", \"label\")\n",
    "fraud_count = fraud_df.count()\n",
    "\n",
    "nonfraud_df = feature_df.filter(\"is_fraud = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced nonfraud dataframe and union with fraud\n",
    "\n",
    "balanced_df = create_balanced_dataframe(nonfraud_df, fraud_count)\n",
    "final_feature_df = fraud_df.unionAll(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "total data count is 311\n",
      "count where label matches prediction 308\n"
     ]
    }
   ],
   "source": [
    "# Based on the balanced dataframe create prediction model and save the model\n",
    "random_forest_model = create_random_forest_model(final_feature_df)\n",
    "random_forest_model.write().overwrite().save(cfg.model.predict.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
