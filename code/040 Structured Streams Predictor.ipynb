{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demolib\n",
    "from demolib import spark, cfg, schema\n",
    "from demolib.streams import *\n",
    "from demolib.mongo import *\n",
    "from demolib.udf import *\n",
    "from demolib.lib import *\n",
    "from time import sleep\n",
    "\n",
    "from pyspark.sql.functions import from_json, col, expr, lit, broadcast\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = mongo_read(cfg.db.customer).withColumn('age', udf_age()).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", cfg.kafka.config['bootstrap.servers']) \\\n",
    "    .option(\"subscribe\", cfg.kafka.topic) \\\n",
    "    .option(\"group.id\", cfg.kafka.groupid) \\\n",
    "    .load() \\\n",
    "    .withColumn('transaction', from_json(col('value').cast(\"string\"), schema.event_transaction.schema)) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_stream = raw_stream \\\n",
    "    .selectExpr(\"transaction.*\", \"partition\", \"offset\") \\\n",
    "    .withColumn(\"amt\", lit(col(\"amt\")).cast(\"double\")) \\\n",
    "    .withColumn(\"merch_lat\", lit(col(\"merch_lat\")).cast(\"double\")) \\\n",
    "    .withColumn(\"merch_long\", lit(col(\"merch_long\")).cast(\"double\")) \\\n",
    "    .drop(\"first\") \\\n",
    "    .drop(\"last\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = spark.sql(\"SET spark.sql.autoBroadcastJoinThreshold = 52428800\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_transactions_df = transaction_stream.join(broadcast(customer_df), \"cc_num\") \\\n",
    "    .withColumn(\"distance\", lit(round(udf_dist(col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")), 2))) \\\n",
    "    .select(\"cc_num\", \"trans_num\", to_timestamp(\"trans_time\", \"yyyy-MM-dd HH:mm:ss\").alias(\"trans_time\"), \"category\", \"merchant\", \"amt\", \"merch_lat\", \"merch_long\", \"distance\", \"age\", \"partition\", \"offset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_transformer_model = PipelineModel.load(cfg.model.preprocessing.path)\n",
    "feature_df = preprocessing_transformer_model.transform(processed_transactions_df)\n",
    "\n",
    "random_forest_model = RandomForestClassificationModel.load(cfg.model.predict.path)\n",
    "prediction_df =  random_forest_model.transform(feature_df).withColumnRenamed(\"prediction\", \"is_fraud\") \\\n",
    "    .select(\"cc_num\", \"trans_num\", \"trans_time\", \"category\", \"merchant\", \"amt\", \"merch_lat\", \"merch_long\", \"distance\", \"age\", \"is_fraud\") \\\n",
    "    .withColumn(\"_id\", col(\"trans_num\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_prediction_df = prediction_df.filter('is_fraud = 1.0')\n",
    "nonfraud_prediction_df = prediction_df.filter('is_fraud = 0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreachBatch_sink(dataset_name):\n",
    "    \"\"\"Returns a function which can be used as callback to foreachBatch to persist specified dataset.\n",
    "    \"\"\"\n",
    "    def persist_batch(batchDF, batch_id):\n",
    "        batchDF.persist()\n",
    "        mongo_save(batchDF, dataset_name, database=cfg.db.name, uri=cfg.db.uri)\n",
    "        batchDF.write \\\n",
    "            .mode('append') \\\n",
    "            .format('json').save(f'../data/out/{dataset_name}')\n",
    "        batchDF.unpersist()\n",
    "    return persist_batch\n",
    "\n",
    "def get_write_stream(df, dataset_name):\n",
    "    \"\"\"Create streaming query with foreachBatch sink to persist stream to dataset\"\"\"\n",
    "    query = df.writeStream \\\n",
    "        .queryName(f'sink__{dataset_name}') \\\n",
    "        .trigger(processingTime = '0 seconds') \\\n",
    "        .foreachBatch(foreachBatch_sink(dataset_name)) \\\n",
    "        .outputMode(\"append\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist `fraud` and `nonfraud` Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_write_stream(nonfraud_prediction_df, 'predict_nonfraud').start()\n",
    "_ = get_write_stream(fraud_prediction_df, 'predict_fraud').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sink__predict_nonfraud', 'sink__predict_fraud']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streams_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to be able to stop the streaming job in graceful manner. For this purpose we use \"flags\". Flags are actually files on the filesystem.\n",
    "\n",
    "Presence of a file means a flag is set.\n",
    "\n",
    "For example if we want to set flag `predictor_stop`, we need to create a file named `__predictor_stop__`. After flag is processed it is cleared (the `clear_on_poll` argument to `flag_poll` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query sink__predict_nonfraud stopped\n",
      "Query sink__predict_fraud stopped\n"
     ]
    }
   ],
   "source": [
    "# Wait for all streams to stop or to observe 'predictor_stop' flag set.\n",
    "while streams_list() and not flag_poll('predictor_stop', clear_on_poll=True):\n",
    "    sleep(2)\n",
    "    \n",
    "# Make sure all streams are stopped.\n",
    "streams_stop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
